{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "# some sheet names vary across the three datasets therefore sheet_names list is changed according to the dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path =r\"/Users/nishaalajmera/Documents/MScProject/Data/\"\n",
    "\n",
    "sheet_names = [\"All patient data - 2021-02-11T0\",\"Genotype\",\"Histochemistry\",\"All Heteroplasmy\",\n",
    "               \"Basic Clinical Data\",\"Phenotype\",\"Systemic features\",\"Family History\",\"Social History\",\n",
    "               \"Blood Results\",\"Neurophysiology\",\"Imaging\",\"Cardiac\",\"Opthalmology\",\"Audiology\",\n",
    "               \"Transplant\",\"NMDAS\",\"Paediatric Information\",\"NPMDS\",\"Mortality\"]\n",
    "\n",
    "dataframes_ncl = [pd.read_excel(os.path.join(path,\"Leighs_patients_at_Newcastle_and_outreach centres_after_Albert_review.xlsx\"),sheet_name=f) for f in sheet_names]\n",
    "\n",
    "\n",
    "sheet_names = [\"All patient data - 2021-02-11T1\",\"Genotype\",\"Histochemistry\",\"All heteroplasmy\",\n",
    "               \"Basic clinical data\",\"Phenotype\",\"Systemic features\",\"Family History\",\"Social History\",\n",
    "               \"Blood results\",\"Neurophysiology\",\"Imaging\",\"Cardiac\",\"Opthalmology\",\"Audiology\",\n",
    "               \"Transplant\",\"NMDAS\",\"Paediatric Information\",\"NPMDS\",\"Mortality\"]\n",
    "\n",
    "dataframes_ucl = [pd.read_excel(os.path.join(path,\"Leighs_patients_at_UCL_after_Albert_review.xlsx\"),sheet_name=f) for f in sheet_names]\n",
    "\n",
    "sheet_names = [\"All patient data - 2021-02-11T1\",\"Genotype\",\"Histochemistry\",\"All Heteroplasmy\",\n",
    "               \"Basic clinical data\",\"Phenotype\",\"Systemic features\",\"Family History\",\"Social history\",\n",
    "               \"Blood Results\",\"Neurophysiology\",\"Imaging\",\"Cardiac\",\"Opthalmology\",\"Audiology\",\n",
    "               \"Transplant\",\"NMDAS\",\"Paediatric Information\",\"NPMDS\",\"Mortality\"]\n",
    "\n",
    "dataframes_oxf = [pd.read_excel(os.path.join(path,\"Leighs_patients_at_Oxford_after_Albert_review.xlsx\"),sheet_name=f) for f in sheet_names]\n",
    "\n",
    "# the dataframes are seperated into individual dataframes for each sheet \n",
    "all_ncl,gen_ncl,his_ncl,heter_ncl,clin_ncl,phen_ncl,sys_ncl,fam_ncl,soc_ncl,blood_ncl,neuro_ncl,img_ncl,car_ncl,opth_ncl,aud_ncl,trans_ncl,nmdas_ncl,paed_ncl,npmds_ncl,mort_ncl = dataframes_ncl[:]\n",
    "\n",
    "all_ucl,gen_ucl,his_ucl,heter_ucl,clin_ucl,phen_ucl,sys_ucl,fam_ucl,soc_ucl,blood_ucl,neuro_ucl,img_ucl,car_ucl,opth_ucl,aud_ucl,trans_ucl,nmdas_ucl,paed_ucl,npmds_ucl,mort_ucl = dataframes_ucl[:]\n",
    "\n",
    "all_oxf,gen_oxf,his_oxf,heter_oxf,clin_oxf,phen_oxf,sys_oxf,fam_oxf,soc_oxf,blood_oxf,neuro_oxf,img_oxf,car_oxf,opth_oxf,aud_oxf,trans_oxf,nmdas_oxf,paed_oxf,npmds_oxf,mort_oxf = dataframes_oxf[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same sheet types are concatenated into one dataframe per sheet \n",
    "all =  pd.concat([all_ncl,all_ucl,all_oxf],ignore_index=True)\n",
    "gen =  pd.concat([gen_ncl,gen_ucl,gen_oxf],ignore_index=True)\n",
    "his =  pd.concat([his_ncl,his_ucl,his_oxf],ignore_index=True)\n",
    "heter = pd.concat([heter_ncl,heter_ucl,heter_oxf],ignore_index=True)\n",
    "clin = pd.concat([clin_ncl,clin_ucl,clin_oxf],ignore_index=True)\n",
    "phen = pd.concat([phen_ncl,phen_ucl,phen_oxf],ignore_index=True)\n",
    "sys = pd.concat([sys_ncl,sys_ucl,sys_oxf],ignore_index=True)\n",
    "fam = pd.concat([fam_ncl,fam_ucl,fam_oxf],ignore_index=True)\n",
    "soc = pd.concat([soc_ncl,soc_ucl,soc_oxf],ignore_index=True)\n",
    "blood = pd.concat([blood_ncl,blood_ucl,blood_oxf],ignore_index=True)\n",
    "neuro = pd.concat([neuro_ncl,neuro_ucl,neuro_oxf],ignore_index=True)\n",
    "img = pd.concat([img_ncl,img_ucl,img_oxf],ignore_index=True)\n",
    "car = pd.concat([car_ncl,car_ucl,car_oxf],ignore_index=True)\n",
    "opth = pd.concat([opth_ncl,opth_ucl,opth_oxf],ignore_index=True)\n",
    "aud = pd.concat([aud_ncl,aud_ucl,aud_oxf],ignore_index=True)\n",
    "trans = pd.concat([trans_ncl,trans_ucl,trans_oxf],ignore_index=True)\n",
    "nmdas = pd.concat([nmdas_ncl,nmdas_ucl,nmdas_oxf],ignore_index=True)\n",
    "paed = pd.concat([paed_ncl,paed_ucl,paed_oxf],ignore_index=True)\n",
    "npmds = pd.concat([npmds_ncl,npmds_ucl,npmds_oxf],ignore_index=True)\n",
    "mort = pd.concat([mort_ncl,mort_ucl,mort_oxf],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the date columns for sheets that are not converted to datetime format are converted \n",
    "\n",
    "df_date = [his,heter,clin,phen,sys,paed]\n",
    "\n",
    "for df in df_date:\n",
    "    df['Date'] = pd.to_datetime(df['Date'],errors='coerce')\n",
    "    \n",
    "# Remove rows that have NA for Date columns in each dataset\n",
    "df_date = [his,heter,clin,phen,sys,blood,neuro,car,opth,aud]\n",
    "\n",
    "for df in df_date:\n",
    "    df.dropna( how='any',subset=['Date'],inplace=True)\n",
    "\n",
    "# Other column that contains notes from Genetics data is removed\n",
    "gen.drop(columns = [\"Other notes\"],inplace=True)\n",
    "\n",
    "# Age from Heterogenous column is removed as it can be calculated from columns \n",
    "heter.drop(columns = [\"Age\"],inplace=True)\n",
    "\n",
    "# Three columns in Histochemistry sheet that have more than 60% missingness are removed\n",
    "his.drop(columns = [\"Histochemistry\",\"COX deficient fibers (%)\",\"RRF (%)\"],inplace=True)\n",
    "\n",
    "# Drop redundant Sex column in clin dataframe\n",
    "clin.drop(columns = [\"Sex\"],inplace=True)\n",
    "\n",
    "\n",
    "# Uninformative column from Neurophysiology data is removed\n",
    "neuro.drop(columns = [\"EEG Report\"],inplace=True)\n",
    "\n",
    "# Date columns for time-independent features are removed\n",
    "paed.drop(columns = [\"Date\"],inplace=True)\n",
    "\n",
    "fam.drop(columns = [\"Date\"],inplace=True)\n",
    "\n",
    "# dropping columns with all NA in mortality dataset\n",
    "mort.dropna(axis=1, how='all',inplace=True)\n",
    "\n",
    "# use replace\n",
    "all.rename(columns = {'Unique ID':'UniqueID','Birth date':'Birth_date', 'Death date':'Death_date'},inplace=True)\n",
    "\n",
    "gen.rename(columns = {'Unique ID':'UniqueID'},inplace=True)\n",
    "\n",
    "his.rename(columns = {'Unique ID':'UniqueID','Normal':'His_Normal','Complex I':'ComplexI','Complex II':'ComplexII','Complex III':'ComplexIII','Complex IV':'ComplexIV','Complex V':'ComplexV'},inplace=True)\n",
    "\n",
    "heter.rename(columns = {'Unique ID':'UniqueID'},inplace=True)\n",
    "\n",
    "clin.rename(columns = {'Unique ID':'UniqueID'},inplace=True)\n",
    "\n",
    "phen.rename(columns = {'Unique ID':'UniqueID'},inplace=True)\n",
    "\n",
    "sys.rename(columns = {'Unique ID':'UniqueID'} ,inplace=True)\n",
    "\n",
    "blood.rename(columns = {'Unique ID':'UniqueID','Hemoglobin (Hb) (g/L)':'Hemoglobin', 'Sodium (Na)':'Sodium',\n",
    "                 'Potassium (K)':'Potassium','Alanine aminotransferase (ALT)':'Alanine_aminotransferase'}, inplace=True)\n",
    "\n",
    "neuro.rename(columns = {'Unique ID':'UniqueID'} ,inplace=True)\n",
    "\n",
    "car.rename(columns = {'Unique ID':'UniqueID','Normal':'ECG_Normal', 'Normal.1':'ECHO_Normal'}, inplace=True)\n",
    "\n",
    "opth.rename(columns = {'Unique ID':'UniqueID','Normal':'Opth_Normal'}, inplace=True)\n",
    "\n",
    "aud.rename(columns = {'Unique ID':'UniqueID'}, inplace=True)\n",
    "\n",
    "paed.rename(columns = {'Unique ID':'UniqueID', 'Diagnosis age':'Diagnosis_age',\n",
    "                'Diagnosis_basis':'Diagnosis_basis','Gestational age':'Gestational_age',\n",
    "                'Delivery method':'Delivery_method'}, inplace=True)\n",
    "\n",
    "mort.rename(columns = {'Unique ID':'UniqueID','Age at death':'Age_at_death','Cause of Death Ia)':'Cause_of_Death_Ia'},inplace=True)\n",
    "\n",
    "fam.rename(columns = {'Unique ID':'UniqueID'}, inplace=True)\n",
    "\n",
    "# Replacing non-numerical value for numerical columns\n",
    "blood['Bilirubin'] = blood['Bilirubin'].replace(['<3'],0)\n",
    "\n",
    "\n",
    "all.loc[all.UniqueID == 'ee9e', 'Sex'] = 'F' #replace the sex of patient ee9e\n",
    "\n",
    "his[['His_Normal','ComplexI','ComplexII','ComplexIII','ComplexIV','ComplexV']] = his[['His_Normal','ComplexI','ComplexII','ComplexIII','ComplexIV','ComplexV']].fillna(0)\n",
    "car[['ECG_Normal','ECHO_Normal']] = car[['ECG_Normal','ECHO_Normal']].fillna(0) \n",
    "opth[['Opth_Normal']] = opth[['Opth_Normal']].fillna(0) \n",
    "\n",
    "# Combining dataframes with temporal clinical outcomes\n",
    "temporal = pd.merge(his, heter, on=[\"UniqueID\", \"Date\"],how='outer')\n",
    "temporal = pd.merge(temporal, clin, on=[\"UniqueID\", \"Date\"],how='outer')\n",
    "temporal = pd.merge(temporal, phen, on=[\"UniqueID\", \"Date\"],how='outer')\n",
    "temporal = pd.merge(temporal, sys, on=[\"UniqueID\", \"Date\"],how='outer')\n",
    "temporal = pd.merge(temporal, blood, on=[\"UniqueID\", \"Date\"],how='outer')\n",
    "temporal = pd.merge(temporal, neuro, on=[\"UniqueID\", \"Date\"],how='outer')\n",
    "temporal = pd.merge(temporal, car, on=[\"UniqueID\", \"Date\"],how='outer')\n",
    "temporal = pd.merge(temporal, opth, on=[\"UniqueID\", \"Date\"],how='outer')\n",
    "temporal = pd.merge(temporal, aud, on=[\"UniqueID\", \"Date\"],how='outer')\n",
    "\n",
    "# Combining dataframes for time-independent patient characteristics\n",
    "char = pd.merge(all, gen, on=[\"UniqueID\"],how='left')\n",
    "char = pd.merge(char, paed, on=[\"UniqueID\"],how='left')\n",
    "char = pd.merge(char, fam, on=[\"UniqueID\"],how='left')\n",
    "\n",
    "# Merging both datasets together\n",
    "df_combined =  pd.merge(char,temporal, on=[\"UniqueID\"],how='left')\n",
    "\n",
    "limitPer = len(df_combined) * .60\n",
    "df_combined.dropna(axis=1,thresh=limitPer,inplace=True)\n",
    "df_combined.drop(columns=['Gestational_age','Delivery_method'],inplace=True)\n",
    "\n",
    "df_condensed = df_combined\n",
    "# Informative imputation\n",
    "df_condensed['Genetics']=df_condensed['Genetics'].fillna(9)\n",
    "df_condensed = pd.merge(df_condensed, all[['UniqueID','Death_date']], on=[\"UniqueID\"],how='left')\n",
    "\n",
    "# Preparing data for survival analysis \n",
    "surv_df = df_condensed\n",
    "\n",
    "# Removing records with dates prior to birth date\n",
    "surv_df = surv_df[surv_df[\"Date\"] > surv_df[\"Birth_date\"]] \n",
    "\n",
    "# Keeping last date seen \n",
    "surv_df = surv_df.sort_values(by=\"Date\").drop_duplicates(subset=[\"UniqueID\"], keep=\"last\")\n",
    "\n",
    "# Calculating patient age at date of visit/record\n",
    "surv_df['Age'] = surv_df.apply(lambda x: x['Death_date']-x['Birth_date'] \\\n",
    "                               if(pd.notnull(x['Death_date'])) \\\n",
    "                               else x['Date']- x['Birth_date'], axis = 1)\n",
    "#Converting Age to Years\n",
    "surv_df['Age'] = surv_df['Age'].astype('timedelta64[D]').astype(int)\n",
    "\n",
    "# Recording Event - Death or Right Censored\n",
    "surv_df['Event'] =  surv_df.apply(lambda x: 0 \\\n",
    "                               if(pd.isnull(x['Death_date'])) \\\n",
    "                               else 1, axis = 1)\n",
    "# Dropping unecessary columns \n",
    "surv_df.drop(columns=['Birth_date','Death_date','Date','UniqueID','Institution'],inplace=True)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# creating instance of labelencoder\n",
    "labelencoder = LabelEncoder()\n",
    "# Assigning numerical values and storing in another column\n",
    "surv_df['Sex'] = labelencoder.fit_transform(surv_df['Sex'])\n",
    "\n",
    "surv_df['Genetics']=surv_df['Genetics'].replace({'Unknown':9,'Other':9})\n",
    "\n",
    "surv_df = pd.get_dummies(surv_df,columns=['Genetics'],drop_first=True,prefix='G')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = surv_df.drop(columns = ['Event','Age'])\n",
    "labels = surv_df[['Event','Age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '/Users/nishaalajmera/Documents/MScProject/DeepHit/class_DeepHit.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '/Users/nishaalajmera/Documents/MScProject/DeepHit/get_main.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '/Users/nishaalajmera/Documents/MScProject/DeepHit/import_data.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# DeepHit input settings\n",
    "\n",
    "# Characteristic data format E, T, X\n",
    "event              = np.asarray(labels[['Event']])\n",
    "time               = np.asarray(labels[['Age']])\n",
    "dhdata             = np.asarray(data.iloc[:,0:24])\n",
    "\n",
    "# Standardisation of the data\n",
    "dhdata             = f_get_Normalization(dhdata, 'standard')\n",
    "\n",
    "# Dimension of time horizon of interest (equivalent to the output dimension per risk)\n",
    "# Factor of 1.2 was used in the paper in order to have enough time-horizon\n",
    "num_Category       = int(np.max(time) * 1.2) \n",
    "\n",
    "# Number of events (censoring is not included)\n",
    "num_Event          = int(len(np.unique(event)) - 1) \n",
    "\n",
    "# Input dimension\n",
    "x_dim              = np.shape(dhdata)[1]\n",
    "\n",
    "# Based on the data, mask1 and mask2 needed to calculate the loss functions\n",
    "# To calculate loss 1 - log-likelihood loss\n",
    "mask1              = f_get_fc_mask2(time, event, num_Event, num_Category)\n",
    "# To calculate loss 2 - cause-specific ranking loss\n",
    "mask2              = f_get_fc_mask3(time, -1, num_Category)\n",
    "\n",
    "DIM                = (x_dim)\n",
    "DATA               = (dhdata, time, event)\n",
    "MASK               = (mask1, mask2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This runs random search to find the optimized hyper-parameters using cross-validation\n",
    "\n",
    "INPUTS:\n",
    "    - OUT_ITERATION: # of training/testing splits\n",
    "    - RS_ITERATION: # of random search iteration\n",
    "    - data_mode: mode to select the time-to-event data from \"import_data.py\"\n",
    "    - seed: random seed for training/testing/validation splits\n",
    "    - EVAL_TIMES: list of time-horizons at which the performance is maximized; \n",
    "                  the validation is performed at given EVAL_TIMES (e.g., [12, 24, 36])\n",
    "\n",
    "OUTPUTS:\n",
    "    - \"hyperparameters_log.txt\" is the output\n",
    "    - Once the hyper parameters are optimized, run \"summarize_results.py\" to get the final results.\n",
    "'''\n",
    "import time, datetime, os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# this saves the current hyperparameters\n",
    "def save_logging(dictionary, log_name):\n",
    "    with open(log_name, 'w') as f:\n",
    "        for key, value in dictionary.items():\n",
    "            f.write('%s:%s\\n' % (key, value))\n",
    "\n",
    "# this open can calls the saved hyperparameters\n",
    "def load_logging(filename):\n",
    "    data = dict()\n",
    "    with open(filename) as f:\n",
    "        def is_float(input):\n",
    "            try:\n",
    "                num = float(input)\n",
    "            except ValueError:\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        for line in f.readlines():\n",
    "            if ':' in line:\n",
    "                key,value = line.strip().split(':', 1)\n",
    "                if value.isdigit():\n",
    "                    data[key] = int(value)\n",
    "                elif is_float(value):\n",
    "                    data[key] = float(value)\n",
    "                elif value == 'None':\n",
    "                    data[key] = None\n",
    "                else:\n",
    "                    data[key] = value\n",
    "            else:\n",
    "                pass # deal with bad lines of text here    \n",
    "    return data\n",
    "\n",
    "\n",
    "# this randomly select hyperparamters based on the given list of candidates\n",
    "def get_random_hyperparameters(out_path):\n",
    "    SET_BATCH_SIZE    = [32, 64, 128] #mb_size\n",
    " \n",
    "    SET_LAYERS        = [1,2,3,5] #number of layers\n",
    "    SET_NODES         = [50, 100, 200, 300] #number of nodes\n",
    "\n",
    "    SET_ACTIVATION_FN = ['relu', 'elu', 'tanh'] #non-linear activation functions\n",
    "\n",
    "    SET_ALPHA         = [0.1, 0.5, 1.0, 3.0, 5.0] #alpha values -> log-likelihood loss \n",
    "    SET_BETA          = [0.1, 0.5, 1.0, 3.0, 5.0] #beta values -> ranking loss\n",
    "    SET_GAMMA         = [0.1, 0.5, 1.0, 3.0, 5.0] #gamma values -> calibration loss\n",
    "\n",
    "    new_parser = {'mb_size': SET_BATCH_SIZE[np.random.randint(len(SET_BATCH_SIZE))],\n",
    "\n",
    "                 'iteration': 50000,\n",
    "\n",
    "                 'keep_prob': 0.6,\n",
    "                 'lr_train': 1e-4,\n",
    "\n",
    "                 'h_dim_shared': SET_NODES[np.random.randint(len(SET_NODES))],\n",
    "                 'h_dim_CS': SET_NODES[np.random.randint(len(SET_NODES))],\n",
    "                 'num_layers_shared':SET_LAYERS[np.random.randint(len(SET_LAYERS))],\n",
    "                 'num_layers_CS':SET_LAYERS[np.random.randint(len(SET_LAYERS))],\n",
    "                 'active_fn': SET_ACTIVATION_FN[np.random.randint(len(SET_ACTIVATION_FN))],\n",
    "\n",
    "                 'alpha':1.0, #default (set alpha = 1.0 and change beta and gamma)\n",
    "                 'beta':SET_BETA[np.random.randint(len(SET_BETA))],\n",
    "                 'gamma':0,   #default (no calibration loss)\n",
    "                 # 'alpha':SET_ALPHA[np.random.randint(len(SET_ALPHA))],\n",
    "                 # 'beta':SET_BETA[np.random.randint(len(SET_BETA))],\n",
    "                 # 'gamma':SET_GAMMA[np.random.randint(len(SET_GAMMA))],\n",
    "\n",
    "                 'out_path':out_path}\n",
    "    \n",
    "    return new_parser #outputs the dictionary of the randomly-chosen hyperparamters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
