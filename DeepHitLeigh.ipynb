{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "# some sheet names vary across the three datasets therefore sheet_names list is changed according to the dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path =r\"/Users/nishaalajmera/Documents/MScProject/Data/\"\n",
    "\n",
    "sheet_names = [\"All patient data - 2021-02-11T0\",\"Genotype\",\"Histochemistry\",\"All Heteroplasmy\",\n",
    "               \"Basic Clinical Data\",\"Phenotype\",\"Systemic features\",\"Family History\",\"Social History\",\n",
    "               \"Blood Results\",\"Neurophysiology\",\"Imaging\",\"Cardiac\",\"Opthalmology\",\"Audiology\",\n",
    "               \"Transplant\",\"NMDAS\",\"Paediatric Information\",\"NPMDS\",\"Mortality\"]\n",
    "\n",
    "dataframes_ncl = [pd.read_excel(os.path.join(path,\"Leighs_patients_at_Newcastle_and_outreach centres_after_Albert_review.xlsx\"),sheet_name=f) for f in sheet_names]\n",
    "\n",
    "\n",
    "sheet_names = [\"All patient data - 2021-02-11T1\",\"Genotype\",\"Histochemistry\",\"All heteroplasmy\",\n",
    "               \"Basic clinical data\",\"Phenotype\",\"Systemic features\",\"Family History\",\"Social History\",\n",
    "               \"Blood results\",\"Neurophysiology\",\"Imaging\",\"Cardiac\",\"Opthalmology\",\"Audiology\",\n",
    "               \"Transplant\",\"NMDAS\",\"Paediatric Information\",\"NPMDS\",\"Mortality\"]\n",
    "\n",
    "dataframes_ucl = [pd.read_excel(os.path.join(path,\"Leighs_patients_at_UCL_after_Albert_review.xlsx\"),sheet_name=f) for f in sheet_names]\n",
    "\n",
    "sheet_names = [\"All patient data - 2021-02-11T1\",\"Genotype\",\"Histochemistry\",\"All Heteroplasmy\",\n",
    "               \"Basic clinical data\",\"Phenotype\",\"Systemic features\",\"Family History\",\"Social history\",\n",
    "               \"Blood Results\",\"Neurophysiology\",\"Imaging\",\"Cardiac\",\"Opthalmology\",\"Audiology\",\n",
    "               \"Transplant\",\"NMDAS\",\"Paediatric Information\",\"NPMDS\",\"Mortality\"]\n",
    "\n",
    "dataframes_oxf = [pd.read_excel(os.path.join(path,\"Leighs_patients_at_Oxford_after_Albert_review.xlsx\"),sheet_name=f) for f in sheet_names]\n",
    "\n",
    "# the dataframes are seperated into individual dataframes for each sheet \n",
    "all_ncl,gen_ncl,his_ncl,heter_ncl,clin_ncl,phen_ncl,sys_ncl,fam_ncl,soc_ncl,blood_ncl,neuro_ncl,img_ncl,car_ncl,opth_ncl,aud_ncl,trans_ncl,nmdas_ncl,paed_ncl,npmds_ncl,mort_ncl = dataframes_ncl[:]\n",
    "\n",
    "all_ucl,gen_ucl,his_ucl,heter_ucl,clin_ucl,phen_ucl,sys_ucl,fam_ucl,soc_ucl,blood_ucl,neuro_ucl,img_ucl,car_ucl,opth_ucl,aud_ucl,trans_ucl,nmdas_ucl,paed_ucl,npmds_ucl,mort_ucl = dataframes_ucl[:]\n",
    "\n",
    "all_oxf,gen_oxf,his_oxf,heter_oxf,clin_oxf,phen_oxf,sys_oxf,fam_oxf,soc_oxf,blood_oxf,neuro_oxf,img_oxf,car_oxf,opth_oxf,aud_oxf,trans_oxf,nmdas_oxf,paed_oxf,npmds_oxf,mort_oxf = dataframes_oxf[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same sheet types are concatenated into one dataframe per sheet \n",
    "all =  pd.concat([all_ncl,all_ucl,all_oxf],ignore_index=True)\n",
    "gen =  pd.concat([gen_ncl,gen_ucl,gen_oxf],ignore_index=True)\n",
    "his =  pd.concat([his_ncl,his_ucl,his_oxf],ignore_index=True)\n",
    "heter = pd.concat([heter_ncl,heter_ucl,heter_oxf],ignore_index=True)\n",
    "clin = pd.concat([clin_ncl,clin_ucl,clin_oxf],ignore_index=True)\n",
    "phen = pd.concat([phen_ncl,phen_ucl,phen_oxf],ignore_index=True)\n",
    "sys = pd.concat([sys_ncl,sys_ucl,sys_oxf],ignore_index=True)\n",
    "fam = pd.concat([fam_ncl,fam_ucl,fam_oxf],ignore_index=True)\n",
    "soc = pd.concat([soc_ncl,soc_ucl,soc_oxf],ignore_index=True)\n",
    "blood = pd.concat([blood_ncl,blood_ucl,blood_oxf],ignore_index=True)\n",
    "neuro = pd.concat([neuro_ncl,neuro_ucl,neuro_oxf],ignore_index=True)\n",
    "img = pd.concat([img_ncl,img_ucl,img_oxf],ignore_index=True)\n",
    "car = pd.concat([car_ncl,car_ucl,car_oxf],ignore_index=True)\n",
    "opth = pd.concat([opth_ncl,opth_ucl,opth_oxf],ignore_index=True)\n",
    "aud = pd.concat([aud_ncl,aud_ucl,aud_oxf],ignore_index=True)\n",
    "trans = pd.concat([trans_ncl,trans_ucl,trans_oxf],ignore_index=True)\n",
    "nmdas = pd.concat([nmdas_ncl,nmdas_ucl,nmdas_oxf],ignore_index=True)\n",
    "paed = pd.concat([paed_ncl,paed_ucl,paed_oxf],ignore_index=True)\n",
    "npmds = pd.concat([npmds_ncl,npmds_ucl,npmds_oxf],ignore_index=True)\n",
    "mort = pd.concat([mort_ncl,mort_ucl,mort_oxf],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the date columns for sheets that are not converted to datetime format are converted \n",
    "\n",
    "df_date = [his,heter,clin,phen,sys,paed]\n",
    "\n",
    "for df in df_date:\n",
    "    df['Date'] = pd.to_datetime(df['Date'],errors='coerce')\n",
    "    \n",
    "# Remove rows that have NA for Date columns in each dataset\n",
    "df_date = [his,heter,clin,phen,sys,blood,neuro,car,opth,aud]\n",
    "\n",
    "for df in df_date:\n",
    "    df.dropna( how='any',subset=['Date'],inplace=True)\n",
    "\n",
    "# Other column that contains notes from Genetics data is removed\n",
    "gen.drop(columns = [\"Other notes\"],inplace=True)\n",
    "\n",
    "# Age from Heterogenous column is removed as it can be calculated from columns \n",
    "heter.drop(columns = [\"Age\"],inplace=True)\n",
    "\n",
    "# Three columns in Histochemistry sheet that have more than 60% missingness are removed\n",
    "his.drop(columns = [\"Histochemistry\",\"COX deficient fibers (%)\",\"RRF (%)\"],inplace=True)\n",
    "\n",
    "# Drop redundant Sex column in clin dataframe\n",
    "clin.drop(columns = [\"Sex\"],inplace=True)\n",
    "\n",
    "\n",
    "# Uninformative column from Neurophysiology data is removed\n",
    "neuro.drop(columns = [\"EEG Report\"],inplace=True)\n",
    "\n",
    "# Date columns for time-independent features are removed\n",
    "paed.drop(columns = [\"Date\"],inplace=True)\n",
    "\n",
    "fam.drop(columns = [\"Date\"],inplace=True)\n",
    "\n",
    "# dropping columns with all NA in mortality dataset\n",
    "mort.dropna(axis=1, how='all',inplace=True)\n",
    "\n",
    "# use replace\n",
    "all.rename(columns = {'Unique ID':'UniqueID','Birth date':'Birth_date', 'Death date':'Death_date'},inplace=True)\n",
    "\n",
    "gen.rename(columns = {'Unique ID':'UniqueID'},inplace=True)\n",
    "\n",
    "his.rename(columns = {'Unique ID':'UniqueID','Normal':'His_Normal','Complex I':'ComplexI','Complex II':'ComplexII','Complex III':'ComplexIII','Complex IV':'ComplexIV','Complex V':'ComplexV'},inplace=True)\n",
    "\n",
    "heter.rename(columns = {'Unique ID':'UniqueID'},inplace=True)\n",
    "\n",
    "clin.rename(columns = {'Unique ID':'UniqueID'},inplace=True)\n",
    "\n",
    "phen.rename(columns = {'Unique ID':'UniqueID'},inplace=True)\n",
    "\n",
    "sys.rename(columns = {'Unique ID':'UniqueID'} ,inplace=True)\n",
    "\n",
    "blood.rename(columns = {'Unique ID':'UniqueID','Hemoglobin (Hb) (g/L)':'Hemoglobin', 'Sodium (Na)':'Sodium',\n",
    "                 'Potassium (K)':'Potassium','Alanine aminotransferase (ALT)':'Alanine_aminotransferase'}, inplace=True)\n",
    "\n",
    "neuro.rename(columns = {'Unique ID':'UniqueID'} ,inplace=True)\n",
    "\n",
    "car.rename(columns = {'Unique ID':'UniqueID','Normal':'ECG_Normal', 'Normal.1':'ECHO_Normal'}, inplace=True)\n",
    "\n",
    "opth.rename(columns = {'Unique ID':'UniqueID','Normal':'Opth_Normal'}, inplace=True)\n",
    "\n",
    "aud.rename(columns = {'Unique ID':'UniqueID'}, inplace=True)\n",
    "\n",
    "paed.rename(columns = {'Unique ID':'UniqueID', 'Diagnosis age':'Diagnosis_age',\n",
    "                'Diagnosis_basis':'Diagnosis_basis','Gestational age':'Gestational_age',\n",
    "                'Delivery method':'Delivery_method'}, inplace=True)\n",
    "\n",
    "mort.rename(columns = {'Unique ID':'UniqueID','Age at death':'Age_at_death','Cause of Death Ia)':'Cause_of_Death_Ia'},inplace=True)\n",
    "\n",
    "fam.rename(columns = {'Unique ID':'UniqueID'}, inplace=True)\n",
    "\n",
    "# Replacing non-numerical value for numerical columns\n",
    "blood['Bilirubin'] = blood['Bilirubin'].replace(['<3'],0)\n",
    "\n",
    "\n",
    "all.loc[all.UniqueID == 'ee9e', 'Sex'] = 'F' #replace the sex of patient ee9e\n",
    "\n",
    "his[['His_Normal','ComplexI','ComplexII','ComplexIII','ComplexIV','ComplexV']] = his[['His_Normal','ComplexI','ComplexII','ComplexIII','ComplexIV','ComplexV']].fillna(0)\n",
    "car[['ECG_Normal','ECHO_Normal']] = car[['ECG_Normal','ECHO_Normal']].fillna(0) \n",
    "opth[['Opth_Normal']] = opth[['Opth_Normal']].fillna(0) \n",
    "\n",
    "# Combining dataframes with temporal clinical outcomes\n",
    "temporal = pd.merge(his, heter, on=[\"UniqueID\", \"Date\"],how='outer')\n",
    "temporal = pd.merge(temporal, clin, on=[\"UniqueID\", \"Date\"],how='outer')\n",
    "temporal = pd.merge(temporal, phen, on=[\"UniqueID\", \"Date\"],how='outer')\n",
    "temporal = pd.merge(temporal, sys, on=[\"UniqueID\", \"Date\"],how='outer')\n",
    "temporal = pd.merge(temporal, blood, on=[\"UniqueID\", \"Date\"],how='outer')\n",
    "temporal = pd.merge(temporal, neuro, on=[\"UniqueID\", \"Date\"],how='outer')\n",
    "temporal = pd.merge(temporal, car, on=[\"UniqueID\", \"Date\"],how='outer')\n",
    "temporal = pd.merge(temporal, opth, on=[\"UniqueID\", \"Date\"],how='outer')\n",
    "temporal = pd.merge(temporal, aud, on=[\"UniqueID\", \"Date\"],how='outer')\n",
    "\n",
    "# Combining dataframes for time-independent patient characteristics\n",
    "char = pd.merge(all, gen, on=[\"UniqueID\"],how='left')\n",
    "char = pd.merge(char, paed, on=[\"UniqueID\"],how='left')\n",
    "char = pd.merge(char, fam, on=[\"UniqueID\"],how='left')\n",
    "\n",
    "# Merging both datasets together\n",
    "df_combined =  pd.merge(char,temporal, on=[\"UniqueID\"],how='left')\n",
    "\n",
    "limitPer = len(df_combined) * .60\n",
    "df_combined.dropna(axis=1,thresh=limitPer,inplace=True)\n",
    "df_combined.drop(columns=['Gestational_age','Delivery_method'],inplace=True)\n",
    "\n",
    "df_condensed = df_combined\n",
    "# Informative imputation\n",
    "df_condensed['Genetics']=df_condensed['Genetics'].fillna(9)\n",
    "df_condensed = pd.merge(df_condensed, all[['UniqueID','Death_date']], on=[\"UniqueID\"],how='left')\n",
    "\n",
    "# Preparing data for survival analysis \n",
    "surv_df = df_condensed\n",
    "\n",
    "# Removing records with dates prior to birth date\n",
    "surv_df = surv_df[surv_df[\"Date\"] > surv_df[\"Birth_date\"]] \n",
    "\n",
    "# Keeping last date seen \n",
    "surv_df = surv_df.sort_values(by=\"Date\").drop_duplicates(subset=[\"UniqueID\"], keep=\"last\")\n",
    "\n",
    "# Calculating patient age at date of visit/record\n",
    "surv_df['Age'] = surv_df.apply(lambda x: x['Death_date']-x['Birth_date'] \\\n",
    "                               if(pd.notnull(x['Death_date'])) \\\n",
    "                               else x['Date']- x['Birth_date'], axis = 1)\n",
    "#Converting Age to Years\n",
    "surv_df['Age'] = surv_df['Age'].astype('timedelta64[D]').astype(int)\n",
    "surv_df['Age'] = surv_df['Age']/365\n",
    "\n",
    "# Recording Event - Death or Right Censored\n",
    "surv_df['Event'] =  surv_df.apply(lambda x: 0 \\\n",
    "                               if(pd.isnull(x['Death_date'])) \\\n",
    "                               else 1, axis = 1)\n",
    "# Dropping unecessary columns \n",
    "surv_df.drop(columns=['Birth_date','Death_date','Date','UniqueID','Institution'],inplace=True)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# creating instance of labelencoder\n",
    "labelencoder = LabelEncoder()\n",
    "# Assigning numerical values and storing in another column\n",
    "surv_df['Sex'] = labelencoder.fit_transform(surv_df['Sex'])\n",
    "\n",
    "surv_df['Genetics']=surv_df['Genetics'].replace({'Unknown':9,'Other':9})\n",
    "\n",
    "surv_df = pd.get_dummies(surv_df,columns=['Genetics'],drop_first=True,prefix='G')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = surv_df.drop(columns = ['Event','Age'])\n",
    "labels = surv_df[['Event','Age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 24)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '/Users/nishaalajmera/Documents/MScProject/DeepHit/utils_network.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '/Users/nishaalajmera/Documents/MScProject/DeepHit/utils_eval.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '/Users/nishaalajmera/Documents/MScProject/DeepHit/class_DeepHit.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '/Users/nishaalajmera/Documents/MScProject/DeepHit/get_main.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '/Users/nishaalajmera/Documents/MScProject/DeepHit/import_data.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '/Users/nishaalajmera/Documents/MScProject/DeepHit/main_RandomSearch.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepHit input settings\n",
    "\n",
    "# Characteristic data format E, T, X\n",
    "event              = np.asarray(labels[['Event']])\n",
    "time               = np.asarray(labels[['Age']])\n",
    "dhdata             = np.asarray(data.iloc[:,0:24])\n",
    "\n",
    "# Standardisation of the data\n",
    "dhdata             = f_get_Normalization(dhdata, 'standard')\n",
    "\n",
    "# Dimension of time horizon of interest (equivalent to the output dimension per risk)\n",
    "# Factor of 1.2 was used in the paper in order to have enough time-horizon\n",
    "num_Category       = int(np.max(time) * 1.2) \n",
    "\n",
    "# Number of events (censoring is not included)\n",
    "num_Event          = int(len(np.unique(event)) - 1) \n",
    "\n",
    "# Input dimension\n",
    "x_dim              = np.shape(dhdata)[1]\n",
    "\n",
    "# Based on the data, mask1 and mask2 needed to calculate the loss functions\n",
    "# To calculate loss 1 - log-likelihood loss\n",
    "mask1              = f_get_fc_mask2(time, event, num_Event, num_Category)\n",
    "# To calculate loss 2 - cause-specific ranking loss\n",
    "mask2              = f_get_fc_mask3(time, -1, num_Category)\n",
    "\n",
    "DIM                = (x_dim)\n",
    "DATA               = (dhdata, time, event)\n",
    "MASK               = (mask1, mask2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 0\n",
      "{'mb_size': 32, 'iteration': 30000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 100, 'num_layers_shared': 2, 'num_layers_CS': 5, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0, 'out_path': 'leigh_syndrome/results'}\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "leigh_syndrome/results/itr_0 (a:1.0 b:5.0 c:0)\n",
      "WARNING:tensorflow:From /Users/nishaalajmera/Documents/MScProject/DeepHit/get_main.py:112: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/nishaalajmera/Documents/MScProject/DeepHit/get_main.py:114: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/nishaalajmera/Documents/MScProject/DeepHit/get_main.py:116: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/nishaalajmera/Documents/MScProject/DeepHit/class_DeepHit.py:69: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/nishaalajmera/Documents/MScProject/DeepHit/class_DeepHit.py:71: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/pytensor/lib/python3.7/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /Users/nishaalajmera/Documents/MScProject/DeepHit/utils_network.py:94: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/nishaalajmera/Documents/MScProject/DeepHit/class_DeepHit.py:120: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/nishaalajmera/Documents/MScProject/DeepHit/class_DeepHit.py:37: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/nishaalajmera/Documents/MScProject/DeepHit/class_DeepHit.py:138: The name tf.diag is deprecated. Please use tf.linalg.tensor_diag instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/nishaalajmera/Documents/MScProject/DeepHit/class_DeepHit.py:144: The name tf.diag_part is deprecated. Please use tf.linalg.tensor_diag_part instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/nishaalajmera/Documents/MScProject/DeepHit/class_DeepHit.py:111: The name tf.losses.get_regularization_loss is deprecated. Please use tf.compat.v1.losses.get_regularization_loss instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/nishaalajmera/Documents/MScProject/DeepHit/class_DeepHit.py:112: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/pytensor/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/nishaalajmera/Documents/MScProject/DeepHit/get_main.py:119: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/nishaalajmera/Documents/MScProject/DeepHit/get_main.py:121: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [4, 10, 30]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m10.6403\u001b[0m\n",
      "updated.... average c-index = 0.6667\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m5.0853\u001b[0m\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m4.8235\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m3.4790\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m2.8534\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m2.6355\u001b[0m\n",
      "updated.... average c-index = 0.6970\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m2.2756\u001b[0m\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m2.1051\u001b[0m\n",
      "updated.... average c-index = 0.7576\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m1.8299\u001b[0m\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m1.7283\u001b[0m\n",
      "updated.... average c-index = 0.9091\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m1.6693\u001b[0m\n",
      "updated.... average c-index = 0.9394\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m1.5186\u001b[0m\n",
      "updated.... average c-index = 0.9697\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m1.3821\u001b[0m\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m1.2994\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m1.2659\u001b[0m\n",
      "|| ITR: 16000 | Loss: \u001b[1m\u001b[33m1.1980\u001b[0m\n",
      "|| ITR: 17000 | Loss: \u001b[1m\u001b[33m1.1371\u001b[0m\n",
      "|| ITR: 18000 | Loss: \u001b[1m\u001b[33m1.1123\u001b[0m\n",
      "Current best: 0.9696969696969697\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 1\n",
      "{'mb_size': 32, 'iteration': 30000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 300, 'num_layers_shared': 5, 'num_layers_CS': 3, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0, 'out_path': 'leigh_syndrome/results'}\n",
      "leigh_syndrome/results/itr_0 (a:1.0 b:1.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [4, 10, 30]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m1.9477\u001b[0m\n",
      "updated.... average c-index = 0.9394\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m1.2936\u001b[0m\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m1.0662\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m0.9308\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m0.8141\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m0.7089\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m0.6895\u001b[0m\n",
      "Current best: 0.9696969696969697\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 2\n",
      "{'mb_size': 8, 'iteration': 30000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 200, 'num_layers_shared': 5, 'num_layers_CS': 3, 'active_fn': 'elu', 'alpha': 1.0, 'beta': 3.0, 'gamma': 0, 'out_path': 'leigh_syndrome/results'}\n",
      "leigh_syndrome/results/itr_0 (a:1.0 b:3.0 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [4, 10, 30]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m1.9549\u001b[0m\n",
      "updated.... average c-index = 0.8485\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m1.5526\u001b[0m\n",
      "updated.... average c-index = 0.9091\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m1.3705\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m1.2979\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m1.1717\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m1.0978\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m1.0850\u001b[0m\n",
      "updated.... average c-index = 0.9697\n",
      "|| ITR: 8000 | Loss: \u001b[1m\u001b[33m1.0481\u001b[0m\n",
      "|| ITR: 9000 | Loss: \u001b[1m\u001b[33m0.9299\u001b[0m\n",
      "updated.... average c-index = 1.0000\n",
      "|| ITR: 10000 | Loss: \u001b[1m\u001b[33m0.9166\u001b[0m\n",
      "|| ITR: 11000 | Loss: \u001b[1m\u001b[33m0.8996\u001b[0m\n",
      "|| ITR: 12000 | Loss: \u001b[1m\u001b[33m0.8660\u001b[0m\n",
      "|| ITR: 13000 | Loss: \u001b[1m\u001b[33m0.8329\u001b[0m\n",
      "|| ITR: 14000 | Loss: \u001b[1m\u001b[33m0.7930\u001b[0m\n",
      "|| ITR: 15000 | Loss: \u001b[1m\u001b[33m0.8464\u001b[0m\n",
      "Current best: 1.0\n",
      "OUTER_ITERATION: 0\n",
      "Random search... itr: 3\n",
      "{'mb_size': 8, 'iteration': 30000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 50, 'num_layers_shared': 2, 'num_layers_CS': 3, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0, 'out_path': 'leigh_syndrome/results'}\n",
      "leigh_syndrome/results/itr_0 (a:1.0 b:0.5 c:0)\n",
      "MAIN TRAINING ...\n",
      "EVALUATION TIMES: [4, 10, 30]\n",
      "|| ITR: 1000 | Loss: \u001b[1m\u001b[33m0.8184\u001b[0m\n",
      "updated.... average c-index = 1.0000\n",
      "|| ITR: 2000 | Loss: \u001b[1m\u001b[33m0.5500\u001b[0m\n",
      "|| ITR: 3000 | Loss: \u001b[1m\u001b[33m0.4771\u001b[0m\n",
      "|| ITR: 4000 | Loss: \u001b[1m\u001b[33m0.4071\u001b[0m\n",
      "|| ITR: 5000 | Loss: \u001b[1m\u001b[33m0.3467\u001b[0m\n",
      "|| ITR: 6000 | Loss: \u001b[1m\u001b[33m0.3040\u001b[0m\n",
      "|| ITR: 7000 | Loss: \u001b[1m\u001b[33m0.2855\u001b[0m\n",
      "Current best: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "# Number of training/validation/test splits during tuning\n",
    "OUT_ITERATION               = 1\n",
    "\n",
    "# Number of random search iterations\n",
    "RS_ITERATION                = 4\n",
    "\n",
    "# For saving purposes of the best parameters\n",
    "data_mode = 'leigh_syndrome'\n",
    "out_path  = data_mode + '/results'\n",
    "\n",
    "# Times when the validation is performed\n",
    "eval_times = [4,10,30]\n",
    "\n",
    "for itr in range(OUT_ITERATION):\n",
    "    \n",
    "    if not os.path.exists(out_path + '/itr_' + str(itr) + '/'):\n",
    "        os.makedirs(out_path + '/itr_' + str(itr) + '/')\n",
    "\n",
    "    max_valid = 0\n",
    "    log_name = out_path + '/itr_' + str(itr) + '/hyperparameters_log.txt'\n",
    "\n",
    "    for r_itr in range(RS_ITERATION):\n",
    "        print('OUTER_ITERATION: ' + str(itr))\n",
    "        print('Random search... itr: ' + str(r_itr))\n",
    "        new_parser = get_random_hyperparameters(out_path)\n",
    "        print(new_parser)\n",
    "\n",
    "        # get validation performance given the hyperparameters\n",
    "        tmp_max = get_valid_performance(DATA, MASK, new_parser, itr, eval_times, MAX_VALUE=max_valid)\n",
    "\n",
    "        if tmp_max > max_valid:\n",
    "            max_valid = tmp_max\n",
    "            max_parser = new_parser\n",
    "            save_logging(max_parser, log_name)  # save the hyperparameters if they provide the maximum validation performance\n",
    "\n",
    "        print('Current best: ' + str(max_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from leigh_syndrome/results//itr_0/models/model_itr_0\n"
     ]
    }
   ],
   "source": [
    "seed=42\n",
    "for out_itr in range(OUT_ITERATION):\n",
    "    # Load the saved optimised hyperparameters\n",
    "    in_path = data_mode + '/results/'\n",
    "    in_hypfile = in_path + '/itr_' + str(out_itr) + '/hyperparameters_log.txt'\n",
    "    in_parser = load_logging(in_hypfile)\n",
    "\n",
    "\n",
    "    # Forward the hyperparameters\n",
    "    mb_size                     = in_parser['mb_size']\n",
    "\n",
    "    iteration                   = in_parser['iteration']\n",
    "\n",
    "    keep_prob                   = in_parser['keep_prob']\n",
    "    lr_train                    = in_parser['lr_train']\n",
    "\n",
    "    h_dim_shared                = in_parser['h_dim_shared']\n",
    "    h_dim_CS                    = in_parser['h_dim_CS']\n",
    "    num_layers_shared           = in_parser['num_layers_shared']\n",
    "    num_layers_CS               = in_parser['num_layers_CS']\n",
    "\n",
    "    if in_parser['active_fn'] == 'relu':\n",
    "        active_fn                = tf.nn.relu\n",
    "    elif in_parser['active_fn'] == 'elu':\n",
    "        active_fn                = tf.nn.elu\n",
    "    elif in_parser['active_fn'] == 'tanh':\n",
    "        active_fn                = tf.nn.tanh\n",
    "    else:\n",
    "        print('Error!')\n",
    "\n",
    "\n",
    "    initial_W                   = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "  # Weights for the loss functions that also can be tuned\n",
    "  # Default values: 1.0\n",
    "    alpha                       = in_parser['alpha']  #for log-likelihood loss\n",
    "    beta                        = in_parser['beta']  #for ranking loss\n",
    "    \n",
    "\n",
    "    # Create the dictionaries \n",
    "    # For the input settings\n",
    "    input_dims                  = { 'x_dim'         : x_dim,\n",
    "                                    'num_Event'     : num_Event,\n",
    "                                    'num_Category'  : num_Category}\n",
    "\n",
    "    # For the hyperparameters\n",
    "    network_settings            = { 'h_dim_shared'         : h_dim_shared,\n",
    "                                    'h_dim_CS'          : h_dim_CS,\n",
    "                                    'num_layers_shared'    : num_layers_shared,\n",
    "                                    'num_layers_CS'    : num_layers_CS,\n",
    "                                    'active_fn'      : active_fn,\n",
    "                                    'initial_W'         : initial_W }\n",
    "\n",
    "\n",
    "    # Create the DeepHit network architecture\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    model = Model_DeepHit(sess, \"DeepHit\", input_dims, network_settings)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training, test sets split\n",
    "    (tr_data,te_data, tr_time,te_time, tr_label,te_label, \n",
    "     tr_mask1,te_mask1, tr_mask2,te_mask2)  = train_test_split(data, time, event, mask1, mask2, test_size=0.20, random_state=seed) \n",
    "    \n",
    "    # Restoring the trained model\n",
    "    saver.restore(sess, in_path + '/itr_' + str(out_itr) + '/models/model_itr_' + str(out_itr))\n",
    "\n",
    "    # Final prediction on the test set \n",
    "    pred = model.predict(te_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "ITR: 1 DATA MODE: leigh_syndrome (a:1.0 b:3.0)\n",
      "SharedNet Parameters: h_dim_shared = 300 num_layers_shared = 5Non-Linearity: <function elu at 0x7fc1a476b440>\n",
      "CSNet Parameters: h_dim_CS = 200 num_layers_CS = 3Non-Linearity: <function elu at 0x7fc1a476b440>\n",
      "--------------------------------------------------------\n",
      "- C-INDEX: \n",
      "         4yr c_index  10yr c_index  30yr c_index\n",
      "Event_1          0.9           0.9           0.9\n",
      "--------------------------------------------------------\n",
      "- BRIER-SCORE: \n",
      "         4yr B_score  10yr B_score  30yr B_score\n",
      "Event_1     0.356239      0.241367       0.00068\n",
      "========================================================\n",
      "========================================================\n",
      "- FINAL C-INDEX: \n",
      "         4yr c_index  10yr c_index  30yr c_index\n",
      "Event_1          0.9           0.9           0.9\n",
      "--------------------------------------------------------\n",
      "- FINAL BRIER-SCORE: \n",
      "         4yr B_score  10yr B_score  30yr B_score\n",
      "Event_1     0.356239      0.241367       0.00068\n",
      "========================================================\n"
     ]
    }
   ],
   "source": [
    "EVAL_TIMES = [4,10,30]\n",
    "FINAL1 = np.zeros([num_Event, len(EVAL_TIMES), OUT_ITERATION])\n",
    "FINAL2 = np.zeros([num_Event, len(EVAL_TIMES), OUT_ITERATION])\n",
    "\n",
    "result1, result2 = np.zeros([num_Event, len(EVAL_TIMES)]), np.zeros([num_Event, len(EVAL_TIMES)])\n",
    "\n",
    "for t, t_time in enumerate(EVAL_TIMES):\n",
    "    eval_horizon = int(t_time)\n",
    "\n",
    "    if eval_horizon >= num_Category:\n",
    "        print( 'ERROR: evaluation horizon is out of range')\n",
    "        result1[:, t] = result2[:, t] = -1\n",
    "    else:\n",
    "        # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "        risk = np.sum(pred[:,:,:(eval_horizon+1)], axis=2) #risk score until EVAL_TIMES\n",
    "        for k in range(num_Event):\n",
    "            # result1[k, t] = c_index(risk[:,k], te_time, (te_label[:,0] == k+1).astype(float), eval_horizon) #-1 for no event (not comparable)\n",
    "            # result2[k, t] = brier_score(risk[:,k], te_time, (te_label[:,0] == k+1).astype(float), eval_horizon) #-1 for no event (not comparable)\n",
    "            result1[k, t] = weighted_c_index(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "            result2[k, t] = weighted_brier_score(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "\n",
    "FINAL1[:, :, out_itr] = result1\n",
    "FINAL2[:, :, out_itr] = result2\n",
    "\n",
    "### SAVE RESULTS\n",
    "row_header = []\n",
    "for t in range(num_Event):\n",
    "    row_header.append('Event_' + str(t+1))\n",
    "\n",
    "col_header1 = []\n",
    "col_header2 = []\n",
    "for t in EVAL_TIMES:\n",
    "    col_header1.append(str(t) + 'yr c_index')\n",
    "    col_header2.append(str(t) + 'yr B_score')\n",
    "\n",
    "# c-index result\n",
    "df1 = pd.DataFrame(result1, index = row_header, columns=col_header1)\n",
    "df1.to_csv(in_path + '/result_CINDEX_itr' + str(out_itr) + '.csv')\n",
    "\n",
    "# brier-score result\n",
    "df2 = pd.DataFrame(result2, index = row_header, columns=col_header2)\n",
    "df2.to_csv(in_path + '/result_BRIER_itr' + str(out_itr) + '.csv')\n",
    "\n",
    "### PRINT RESULTS\n",
    "print('========================================================')\n",
    "print('ITR: ' + str(out_itr+1) + ' DATA MODE: ' + data_mode + ' (a:' + str(alpha) + ' b:' + str(beta) + ')' )\n",
    "print('SharedNet Parameters: ' + 'h_dim_shared = '+str(h_dim_shared) + ' num_layers_shared = '+str(num_layers_shared) + 'Non-Linearity: ' + str(active_fn))\n",
    "print('CSNet Parameters: ' + 'h_dim_CS = '+str(h_dim_CS) + ' num_layers_CS = '+str(num_layers_CS) + 'Non-Linearity: ' + str(active_fn)) \n",
    "\n",
    "print('--------------------------------------------------------')\n",
    "print('- C-INDEX: ')\n",
    "print(df1)\n",
    "print('--------------------------------------------------------')\n",
    "print('- BRIER-SCORE: ')\n",
    "print(df2)\n",
    "print('========================================================')\n",
    "\n",
    "\n",
    "    \n",
    "### FINAL MEAN/STD\n",
    "# c-index result\n",
    "df1_mean = pd.DataFrame(np.mean(FINAL1, axis=2), index = row_header, columns=col_header1)\n",
    "df1_std  = pd.DataFrame(np.std(FINAL1, axis=2), index = row_header, columns=col_header1)\n",
    "df1_mean.to_csv(in_path + '/result_CINDEX_FINAL_MEAN.csv')\n",
    "df1_std.to_csv(in_path + '/result_CINDEX_FINAL_STD.csv')\n",
    "\n",
    "# brier-score result\n",
    "df2_mean = pd.DataFrame(np.mean(FINAL2, axis=2), index = row_header, columns=col_header2)\n",
    "df2_std  = pd.DataFrame(np.std(FINAL2, axis=2), index = row_header, columns=col_header2)\n",
    "df2_mean.to_csv(in_path + '/result_BRIER_FINAL_MEAN.csv')\n",
    "df2_std.to_csv(in_path + '/result_BRIER_FINAL_STD.csv')\n",
    "\n",
    "\n",
    "### PRINT RESULTS\n",
    "print('========================================================')\n",
    "print('- FINAL C-INDEX: ')\n",
    "print(df1_mean)\n",
    "print('--------------------------------------------------------')\n",
    "print('- FINAL BRIER-SCORE: ')\n",
    "print(df2_mean)\n",
    "print('========================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
